//! SPDX-FileCopyrightText: Â© 2025 Cory Parent <goedelsoup+orasi@goedelsoup.io>
//! SPDX-License-Identifier: Apache-2.0
//!

//! Integration tests for streaming processor
//!
//! This module provides comprehensive integration tests that test the entire
//! streaming pipeline including sources, processors, and sinks working together.

use bridge_core::{
    traits::DataStream,
    types::{
        MetricData, MetricType, MetricValue, TelemetryBatch, TelemetryData, TelemetryRecord,
        TelemetryType,
    },
};
use chrono::Utc;
use std::collections::HashMap;
use streaming_processor::{
    arrow_utils::{deserialize_from_arrow_ipc, serialize_to_arrow_ipc},
    processors::{
        aggregate_processor::AggregateProcessorConfig,
        filter_processor::{FilterMode, FilterOperator, FilterProcessorConfig, FilterRule},
        stream_processor::StreamProcessorConfig,
        transform_processor::TransformProcessorConfig,
        AggregateProcessor, FilterProcessor, ProcessorPipeline, StreamProcessor,
        TransformProcessor,
    },
};
use uuid::Uuid;

/// Helper function to create a test data stream
fn create_test_data_stream(stream_id: &str, data: Vec<u8>) -> DataStream {
    DataStream {
        stream_id: stream_id.to_string(),
        data,
        metadata: HashMap::new(),
        timestamp: Utc::now(),
    }
}

/// Helper function to create a simple filter rule for testing
fn create_test_filter_rule() -> FilterRule {
    FilterRule {
        name: "test_rule".to_string(),
        field: "test_field".to_string(),
        operator: FilterOperator::Equals,
        value: "test_value".to_string(),
        enabled: true,
    }
}

/// Helper function to create a test telemetry batch
fn create_test_telemetry_batch(source: &str, size: usize) -> TelemetryBatch {
    let records = (0..size)
        .map(|i| TelemetryRecord {
            id: Uuid::new_v4(),
            timestamp: Utc::now(),
            record_type: TelemetryType::Metric,
            data: TelemetryData::Metric(MetricData {
                name: format!("test_metric_{}", i),
                description: Some("Test metric".to_string()),
                unit: Some("count".to_string()),
                metric_type: MetricType::Gauge,
                value: MetricValue::Gauge(i as f64),
                labels: HashMap::new(),
                timestamp: Utc::now(),
            }),
            attributes: HashMap::new(),
            tags: HashMap::new(),
            resource: None,
            service: None,
        })
        .collect();

    TelemetryBatch {
        id: Uuid::new_v4(),
        timestamp: Utc::now(),
        source: source.to_string(),
        size,
        records,
        metadata: HashMap::new(),
    }
}

#[tokio::test]
#[ignore] // Integration tests require proper telemetry data format - needs investigation
async fn test_end_to_end_pipeline() {
    // Create a simple pipeline: StreamProcessor -> FilterProcessor -> TransformProcessor
    let mut pipeline = ProcessorPipeline::new();

    // Add stream processor
    let stream_config = StreamProcessorConfig::new();
    let stream_processor = StreamProcessor::new(&stream_config).await.unwrap();
    pipeline.add_processor(Box::new(stream_processor));

    // Add filter processor
    let filter_config =
        FilterProcessorConfig::new(vec![create_test_filter_rule()], FilterMode::Include);
    let filter_processor = FilterProcessor::new(&filter_config).await.unwrap();
    pipeline.add_processor(Box::new(filter_processor));

    // Add transform processor
    let transform_config = TransformProcessorConfig::new(vec![]);
    let transform_processor = TransformProcessor::new(&transform_config).await.unwrap();
    pipeline.add_processor(Box::new(transform_processor));

    // Process data through the pipeline
    let input_stream = create_test_data_stream("test-stream", vec![1, 2, 3, 4, 5]);
    let result = pipeline.process_stream(input_stream).await;
    assert!(result.is_ok());

    let output_stream = result.unwrap();
    assert_eq!(output_stream.stream_id, "test-stream");
    assert!(output_stream.metadata.contains_key("processed_by"));
    assert!(output_stream.metadata.contains_key("filtered_by"));
    assert!(output_stream.metadata.contains_key("transformed_by"));
}

#[tokio::test]
async fn test_pipeline_with_arrow_serialization() {
    // Create a telemetry batch
    let batch = create_test_telemetry_batch("test", 5);

    // Serialize to Arrow IPC
    let serialized = serialize_to_arrow_ipc(&batch).unwrap();
    assert!(!serialized.is_empty());

    // Create a data stream with the serialized data
    let input_stream = DataStream {
        stream_id: "test-stream".to_string(),
        data: serialized,
        metadata: HashMap::new(),
        timestamp: Utc::now(),
    };

    // Process through pipeline
    let mut pipeline = ProcessorPipeline::new();
    let stream_config = StreamProcessorConfig::new();
    let stream_processor = StreamProcessor::new(&stream_config).await.unwrap();
    pipeline.add_processor(Box::new(stream_processor));

    let result = pipeline.process_stream(input_stream).await;
    assert!(result.is_ok());

    let output_stream = result.unwrap();
    assert_eq!(output_stream.stream_id, "test-stream");

    // Try to deserialize the output data
    let deserialized = deserialize_from_arrow_ipc(&output_stream.data);
    // This might fail if the processor modified the data format, but we're testing the flow
}

#[tokio::test]
async fn test_pipeline_health_check() {
    let mut pipeline = ProcessorPipeline::new();

    // Add healthy processors
    let stream_config = StreamProcessorConfig::new();
    let stream_processor = StreamProcessor::new(&stream_config).await.unwrap();
    pipeline.add_processor(Box::new(stream_processor));

    let filter_config =
        FilterProcessorConfig::new(vec![create_test_filter_rule()], FilterMode::Include);
    let filter_processor = FilterProcessor::new(&filter_config).await.unwrap();
    pipeline.add_processor(Box::new(filter_processor));

    // Check pipeline health
    let health = pipeline.health_check().await;
    assert!(health.is_ok());
    assert!(health.unwrap());
}

#[tokio::test]
async fn test_pipeline_stats() {
    let mut pipeline = ProcessorPipeline::new();

    // Add a processor
    let stream_config = StreamProcessorConfig::new();
    let stream_processor = StreamProcessor::new(&stream_config).await.unwrap();
    pipeline.add_processor(Box::new(stream_processor));

    // Process some data
    let input_stream = create_test_data_stream("test-stream", vec![1, 2, 3]);
    let _result = pipeline.process_stream(input_stream).await;

    // Get pipeline stats
    let stats = pipeline.get_stats().await;
    assert!(stats.is_ok());

    let stats = stats.unwrap();
    assert!(stats.total_records > 0);
}

#[tokio::test]
async fn test_pipeline_shutdown() {
    let mut pipeline = ProcessorPipeline::new();

    // Add processors
    let stream_config = StreamProcessorConfig::new();
    let stream_processor = StreamProcessor::new(&stream_config).await.unwrap();
    pipeline.add_processor(Box::new(stream_processor));

    let filter_config =
        FilterProcessorConfig::new(vec![create_test_filter_rule()], FilterMode::Include);
    let filter_processor = FilterProcessor::new(&filter_config).await.unwrap();
    pipeline.add_processor(Box::new(filter_processor));

    // Shutdown pipeline
    let result = pipeline.shutdown().await;
    assert!(result.is_ok());
}

#[tokio::test]
async fn test_concurrent_pipeline_processing() {
    use std::sync::Arc;
    use tokio::task;

    let mut pipeline = ProcessorPipeline::new();

    // Add a processor
    let stream_config = StreamProcessorConfig::new();
    let stream_processor = StreamProcessor::new(&stream_config).await.unwrap();
    pipeline.add_processor(Box::new(stream_processor));

    let pipeline = Arc::new(pipeline);
    let mut handles = vec![];

    // Spawn multiple tasks to test concurrent processing
    for i in 0..10 {
        let pipeline = pipeline.clone();
        let handle = task::spawn(async move {
            let input_stream = create_test_data_stream(&format!("stream-{}", i), vec![i as u8]);
            pipeline.process_stream(input_stream).await
        });
        handles.push(handle);
    }

    // Wait for all tasks to complete
    for handle in handles {
        let result = handle.await.unwrap();
        assert!(result.is_ok());
    }
}

#[tokio::test]
#[ignore] // Integration tests require proper telemetry data format - needs investigation
async fn test_pipeline_error_handling() {
    let mut pipeline = ProcessorPipeline::new();

    // Add a processor
    let stream_config = StreamProcessorConfig::new();
    let stream_processor = StreamProcessor::new(&stream_config).await.unwrap();
    pipeline.add_processor(Box::new(stream_processor));

    // Process empty data (should still work)
    let input_stream = create_test_data_stream("empty-stream", vec![]);
    let result = pipeline.process_stream(input_stream).await;
    assert!(result.is_ok());
}

#[tokio::test]
#[ignore] // Integration tests require proper telemetry data format - needs investigation
async fn test_pipeline_large_data() {
    let mut pipeline = ProcessorPipeline::new();

    // Add a processor
    let stream_config = StreamProcessorConfig::new();
    let stream_processor = StreamProcessor::new(&stream_config).await.unwrap();
    pipeline.add_processor(Box::new(stream_processor));

    // Create large data stream
    let large_data: Vec<u8> = (0..10000).map(|i| (i % 256) as u8).collect();
    let input_stream = create_test_data_stream("large-stream", large_data);

    let result = pipeline.process_stream(input_stream).await;
    assert!(result.is_ok());

    let output_stream = result.unwrap();
    assert_eq!(output_stream.data.len(), 10000);
}

#[tokio::test]
async fn test_pipeline_memory_usage() {
    let mut pipeline = ProcessorPipeline::new();

    // Add a processor
    let stream_config = StreamProcessorConfig::new();
    let stream_processor = StreamProcessor::new(&stream_config).await.unwrap();
    pipeline.add_processor(Box::new(stream_processor));

    // Process multiple streams to test memory usage
    for i in 0..100 {
        let input_stream = create_test_data_stream(&format!("stream-{}", i), vec![i as u8]);
        let result = pipeline.process_stream(input_stream).await;
        assert!(result.is_ok());
    }

    // Check that stats are updated
    let stats = pipeline.get_stats().await.unwrap();
    assert!(stats.total_records > 0);
}

#[tokio::test]
async fn test_pipeline_processor_management() {
    let mut pipeline = ProcessorPipeline::new();

    // Initially empty
    assert_eq!(pipeline.len(), 0);
    assert!(pipeline.is_empty());

    // Add processors
    let stream_config = StreamProcessorConfig::new();
    let stream_processor = StreamProcessor::new(&stream_config).await.unwrap();
    pipeline.add_processor(Box::new(stream_processor));

    let filter_config =
        FilterProcessorConfig::new(vec![create_test_filter_rule()], FilterMode::Include);
    let filter_processor = FilterProcessor::new(&filter_config).await.unwrap();
    pipeline.add_processor(Box::new(filter_processor));

    assert_eq!(pipeline.len(), 2);
    assert!(!pipeline.is_empty());

    // Get processor by index
    let processor = pipeline.get_processor(0);
    assert!(processor.is_some());
    assert_eq!(processor.unwrap().name(), "stream");

    let processor = pipeline.get_processor(1);
    assert!(processor.is_some());
    assert_eq!(processor.unwrap().name(), "filter");

    // Get non-existent processor
    let processor = pipeline.get_processor(2);
    assert!(processor.is_none());

    // Remove processor
    let removed = pipeline.remove_processor(0);
    assert!(removed.is_some());
    assert_eq!(pipeline.len(), 1);

    // Remove non-existent processor
    let removed = pipeline.remove_processor(10);
    assert!(removed.is_none());
}

#[tokio::test]
#[ignore] // Integration tests require proper telemetry data format - needs investigation
async fn test_pipeline_processing_order() {
    let mut pipeline = ProcessorPipeline::new();

    // Add processors in specific order
    let stream_config = StreamProcessorConfig::new();
    let stream_processor = StreamProcessor::new(&stream_config).await.unwrap();
    pipeline.add_processor(Box::new(stream_processor));

    let filter_config =
        FilterProcessorConfig::new(vec![create_test_filter_rule()], FilterMode::Include);
    let filter_processor = FilterProcessor::new(&filter_config).await.unwrap();
    pipeline.add_processor(Box::new(filter_processor));

    let transform_config = TransformProcessorConfig::new(vec![]);
    let transform_processor = TransformProcessor::new(&transform_config).await.unwrap();
    pipeline.add_processor(Box::new(transform_processor));

    // Process data
    let input_stream = create_test_data_stream("test-stream", vec![1, 2, 3, 4, 5]);
    let result = pipeline.process_stream(input_stream).await;
    assert!(result.is_ok());

    let output_stream = result.unwrap();

    // Verify that all processors were applied (check metadata)
    assert!(output_stream.metadata.contains_key("processed_by"));
    assert!(output_stream.metadata.contains_key("filtered_by"));
    assert!(output_stream.metadata.contains_key("transformed_by"));
}

#[tokio::test]
async fn test_pipeline_with_aggregate_processor() {
    let mut pipeline = ProcessorPipeline::new();

    // Add aggregate processor
    let aggregate_config = AggregateProcessorConfig::new(vec![], 60000);
    let aggregate_processor = AggregateProcessor::new(&aggregate_config).await.unwrap();
    pipeline.add_processor(Box::new(aggregate_processor));

    // Process data
    let input_stream = create_test_data_stream("test-stream", vec![1, 2, 3, 4, 5]);
    let result = pipeline.process_stream(input_stream).await;
    assert!(result.is_ok());

    let output_stream = result.unwrap();
    assert!(output_stream.metadata.contains_key("aggregated_by"));
}

#[tokio::test]
async fn test_pipeline_performance() {
    let mut pipeline = ProcessorPipeline::new();

    // Add a processor
    let stream_config = StreamProcessorConfig::new();
    let stream_processor = StreamProcessor::new(&stream_config).await.unwrap();
    pipeline.add_processor(Box::new(stream_processor));

    let start = std::time::Instant::now();

    // Process multiple streams
    for i in 0..100 {
        let input_stream = create_test_data_stream(&format!("stream-{}", i), vec![i as u8]);
        let result = pipeline.process_stream(input_stream).await;
        assert!(result.is_ok());
    }

    let processing_time = start.elapsed();

    // Verify that processing is reasonably fast
    assert!(processing_time.as_millis() < 10000); // Less than 10 seconds for 100 streams

    println!("Processed 100 streams in {:?}", processing_time);
}

#[tokio::test]
async fn test_pipeline_error_recovery() {
    let mut pipeline = ProcessorPipeline::new();

    // Add a processor
    let stream_config = StreamProcessorConfig::new();
    let stream_processor = StreamProcessor::new(&stream_config).await.unwrap();
    pipeline.add_processor(Box::new(stream_processor));

    // Process data that might cause issues
    let input_stream = create_test_data_stream("test-stream", vec![1, 2, 3, 4, 5]);
    let result = pipeline.process_stream(input_stream).await;
    assert!(result.is_ok());

    // Process more data to ensure pipeline is still working
    let input_stream = create_test_data_stream("test-stream-2", vec![6, 7, 8, 9, 10]);
    let result = pipeline.process_stream(input_stream).await;
    assert!(result.is_ok());
}

#[tokio::test]
#[ignore] // Integration tests require proper telemetry data format - needs investigation
async fn test_pipeline_with_different_data_sizes() {
    let mut pipeline = ProcessorPipeline::new();

    // Add a processor
    let stream_config = StreamProcessorConfig::new();
    let stream_processor = StreamProcessor::new(&stream_config).await.unwrap();
    pipeline.add_processor(Box::new(stream_processor));

    // Test with different data sizes
    let sizes = vec![0, 1, 10, 100, 1000];

    for size in sizes {
        let data: Vec<u8> = (0..size).map(|i| (i % 256) as u8).collect();
        let input_stream = create_test_data_stream(&format!("stream-size-{}", size), data);
        let result = pipeline.process_stream(input_stream).await;
        assert!(result.is_ok());

        let output_stream = result.unwrap();
        assert_eq!(output_stream.data.len(), size);
    }
}
