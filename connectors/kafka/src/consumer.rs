//! SPDX-FileCopyrightText: Â© 2025 Cory Parent <goedelsoup+orasi@goedelsoup.io>
//! SPDX-License-Identifier: Apache-2.0
//!

//! Apache Kafka consumer implementation
//! 
//! This module provides the Kafka consumer that implements
//! the LakehouseReader trait for reading telemetry data from Kafka.

use std::time::Instant;
use async_trait::async_trait;
use tracing::{debug, info};
use serde_json;
use chrono::{DateTime, Utc};
use uuid::Uuid;
use bridge_core::traits::LakehouseReader;
use bridge_core::types::{MetricsQuery, TracesQuery, LogsQuery, MetricsResult, TracesResult, LogsResult, QueryStatus, QueryError};
use bridge_core::error::BridgeResult;

use crate::config::KafkaConfig;
use crate::error::{KafkaError as ConnectorKafkaError, KafkaResult};

/// Kafka consumer implementation
pub struct KafkaConsumer {
    /// Kafka configuration
    config: KafkaConfig,
    /// Consumer state
    initialized: bool,
    /// Statistics
    stats: ConsumerStats,
}

/// Consumer statistics
#[derive(Debug, Clone)]
struct ConsumerStats {
    total_reads: u64,
    total_records: u64,
    total_bytes: u64,
    error_count: u64,
    last_read_time: Option<DateTime<Utc>>,
    total_read_time_ms: u64,
}

impl Default for ConsumerStats {
    fn default() -> Self {
        Self {
            total_reads: 0,
            total_records: 0,
            total_bytes: 0,
            error_count: 0,
            last_read_time: None,
            total_read_time_ms: 0,
        }
    }
}

impl KafkaConsumer {
    /// Create a new Kafka consumer
    pub async fn new(config: KafkaConfig) -> KafkaResult<Self> {
        info!("Creating Kafka consumer for topic: {}", config.topic_name());
        
        let consumer = Self {
            config,
            initialized: true,
            stats: ConsumerStats::default(),
        };
        
        info!("Kafka consumer initialized successfully");
        Ok(consumer)
    }

    /// Get the configuration
    pub fn config(&self) -> &KafkaConfig {
        &self.config
    }

    /// Check if the consumer is initialized
    pub fn is_initialized(&self) -> bool {
        self.initialized
    }

    /// Simulate polling messages from Kafka
    async fn poll_messages(&mut self, _timeout_ms: u64) -> KafkaResult<Vec<String>> {
        let start_time = Instant::now();
        
        // Simulate Kafka poll operation
        self.stats.total_reads += 1;
        self.stats.last_read_time = Some(Utc::now());
        
        // Return some sample messages for testing
        let sample_messages = vec![
            r#"{"name":"cpu_usage","value":75.5,"timestamp":"2024-01-01T00:00:00Z","labels":{"service":"web-server"}}"#.to_string(),
            r#"{"name":"memory_usage","value":60.2,"timestamp":"2024-01-01T00:00:01Z","labels":{"service":"web-server"}}"#.to_string(),
        ];
        
        // Track the read time
        let read_time_ms = start_time.elapsed().as_millis() as u64;
        self.stats.total_read_time_ms += read_time_ms;
        
        Ok(sample_messages)
    }
}

#[async_trait]
impl LakehouseReader for KafkaConsumer {
    async fn query_metrics(&self, query: MetricsQuery) -> BridgeResult<MetricsResult> {
        debug!("Querying metrics from Kafka: {:?}", query);
        
        let start_time = Instant::now();
        let mut metrics = Vec::new();
        let mut errors = Vec::new();
        
        // Create a mutable copy for polling messages
        let mut consumer_copy = self.clone();
        
        // Poll messages from Kafka
        let messages = match consumer_copy.poll_messages(5000).await {
            Ok(msgs) => msgs,
            Err(e) => {
                errors.push(QueryError::new("KAFKA_ERROR".to_string(), format!("Failed to poll messages: {}", e)));
                return Ok(MetricsResult::new(Uuid::new_v4(), vec![])
                    .with_status(QueryStatus::Failed)
                    .with_duration(start_time.elapsed().as_millis() as u64)
                    .with_error(errors[0].clone()));
            }
        };
        
        for message in messages {
            if let Ok(metric) = serde_json::from_str::<bridge_core::types::MetricData>(&message) {
                // Apply query filters if specified
                // For now, include all metrics since we don't have proper time range filtering
                let should_include = true;
                
                if should_include {
                    metrics.push(metric);
                    // Note: We can't update stats here since self is immutable
                    // In a real implementation, you'd need to handle this differently
                }
            }
        }
        
        let duration_ms = start_time.elapsed().as_millis() as u64;
        let status = if errors.is_empty() {
            QueryStatus::Success
        } else {
            QueryStatus::Partial
        };
        
        info!("Successfully queried {} metrics from Kafka", metrics.len());
        
        Ok(MetricsResult::new(Uuid::new_v4(), metrics)
            .with_status(status)
            .with_duration(duration_ms))
    }

    async fn query_traces(&self, query: TracesQuery) -> BridgeResult<TracesResult> {
        debug!("Querying traces from Kafka: {:?}", query);
        
        let start_time = Instant::now();
        let mut traces = Vec::new();
        let mut errors = Vec::new();
        
        // Create a mutable copy for polling messages
        let mut consumer_copy = self.clone();
        
        // Poll messages from Kafka
        let messages = match consumer_copy.poll_messages(5000).await {
            Ok(msgs) => msgs,
            Err(e) => {
                errors.push(QueryError::new("KAFKA_ERROR".to_string(), format!("Failed to poll messages: {}", e)));
                return Ok(TracesResult::new(Uuid::new_v4(), vec![])
                    .with_status(QueryStatus::Failed)
                    .with_duration(start_time.elapsed().as_millis() as u64)
                    .with_error(errors[0].clone()));
            }
        };
        
        for message in messages {
            if let Ok(trace) = serde_json::from_str::<bridge_core::types::TraceData>(&message) {
                // Apply query filters if specified
                // For now, include all traces since we don't have proper time range filtering
                let should_include = true;
                
                if should_include {
                    traces.push(trace);
                    // Note: We can't update stats here since self is immutable
                    // In a real implementation, you'd need to handle this differently
                }
            }
        }
        
        let duration_ms = start_time.elapsed().as_millis() as u64;
        let status = if errors.is_empty() {
            QueryStatus::Success
        } else {
            QueryStatus::Partial
        };
        
        info!("Successfully queried {} traces from Kafka", traces.len());
        
        Ok(TracesResult::new(Uuid::new_v4(), traces)
            .with_status(status)
            .with_duration(duration_ms))
    }

    async fn query_logs(&self, query: LogsQuery) -> BridgeResult<LogsResult> {
        debug!("Querying logs from Kafka: {:?}", query);
        
        let start_time = Instant::now();
        let mut logs = Vec::new();
        let mut errors = Vec::new();
        
        // Create a mutable copy for polling messages
        let mut consumer_copy = self.clone();
        
        // Poll messages from Kafka
        let messages = match consumer_copy.poll_messages(5000).await {
            Ok(msgs) => msgs,
            Err(e) => {
                errors.push(QueryError::new("KAFKA_ERROR".to_string(), format!("Failed to poll messages: {}", e)));
                return Ok(LogsResult::new(Uuid::new_v4(), vec![])
                    .with_status(QueryStatus::Failed)
                    .with_duration(start_time.elapsed().as_millis() as u64)
                    .with_error(errors[0].clone()));
            }
        };
        
        for message in messages {
            if let Ok(log) = serde_json::from_str::<bridge_core::types::LogData>(&message) {
                // Apply query filters if specified
                // For now, include all logs since we don't have proper time range filtering
                let should_include = true;
                
                if should_include {
                    logs.push(log);
                    // Note: We can't update stats here since self is immutable
                    // In a real implementation, you'd need to handle this differently
                }
            }
        }
        
        let duration_ms = start_time.elapsed().as_millis() as u64;
        let status = if errors.is_empty() {
            QueryStatus::Success
        } else {
            QueryStatus::Partial
        };
        
        info!("Successfully queried {} logs from Kafka", logs.len());
        
        Ok(LogsResult::new(Uuid::new_v4(), logs)
            .with_status(status)
            .with_duration(duration_ms))
    }

    async fn execute_query(&self, query: String) -> BridgeResult<serde_json::Value> {
        debug!("Executing custom query on Kafka: {}", query);
        
        let start_time = Instant::now();
        let mut results = Vec::new();
        
        // Create a mutable copy for polling messages
        let mut consumer_copy = self.clone();
        
        // Poll messages from Kafka
        let messages = match consumer_copy.poll_messages(5000).await {
            Ok(msgs) => msgs,
            Err(e) => {
                return Ok(serde_json::json!({
                    "status": "error",
                    "message": format!("Failed to poll messages: {}", e),
                    "data": [],
                    "query_time_ms": start_time.elapsed().as_millis() as u64,
                    "total_count": 0
                }));
            }
        };
        
        for message in messages {
            if let Ok(json_value) = serde_json::from_str::<serde_json::Value>(&message) {
                results.push(json_value);
                // Note: We can't update stats here since self is immutable
                // In a real implementation, you'd need to handle this differently
            }
        }
        
        let query_time_ms = start_time.elapsed().as_millis() as u64;
        
        info!("Successfully executed custom query on Kafka, returned {} results", results.len());
        
        Ok(serde_json::json!({
            "status": "success",
            "message": "Query executed successfully",
            "data": results,
            "query_time_ms": query_time_ms,
            "total_count": results.len()
        }))
    }



    async fn get_stats(&self) -> BridgeResult<bridge_core::traits::ReaderStats> {
        let reads_per_minute = if let Some(last_read) = self.stats.last_read_time {
            let elapsed = Utc::now().signed_duration_since(last_read);
            if elapsed.num_seconds() > 0 {
                (self.stats.total_reads * 60) / elapsed.num_seconds() as u64
            } else {
                0
            }
        } else {
            0
        };
        
        let records_per_minute = if let Some(last_read) = self.stats.last_read_time {
            let elapsed = Utc::now().signed_duration_since(last_read);
            if elapsed.num_seconds() > 0 {
                (self.stats.total_records * 60) / elapsed.num_seconds() as u64
            } else {
                0
            }
        } else {
            0
        };
        
        let avg_read_time_ms = if self.stats.total_reads > 0 {
            self.stats.total_read_time_ms as f64 / self.stats.total_reads as f64
        } else {
            0.0
        };
        
        Ok(bridge_core::traits::ReaderStats {
            total_reads: self.stats.total_reads,
            total_records: self.stats.total_records,
            reads_per_minute,
            records_per_minute,
            avg_read_time_ms,
            error_count: self.stats.error_count,
            last_read_time: self.stats.last_read_time,
        })
    }

    async fn close(&self) -> BridgeResult<()> {
        info!("Closing Kafka consumer");
        info!("Kafka consumer closed successfully");
        Ok(())
    }
}

impl Clone for KafkaConsumer {
    fn clone(&self) -> Self {
        Self {
            config: self.config.clone(),
            initialized: self.initialized,
            stats: self.stats.clone(),
        }
    }
}
