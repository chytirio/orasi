//! SPDX-FileCopyrightText: Â© 2025 Cory Parent <goedelsoup+orasi@goedelsoup.io>
//! SPDX-License-Identifier: Apache-2.0
//!

//! Delta Lake writer implementation
//! 
//! This module provides a real Delta Lake writer that creates actual Delta Lake
//! table files and metadata, working with our current system.

use std::collections::HashMap;
use std::path::Path;
use std::sync::Arc;
use tokio::sync::RwLock;
use async_trait::async_trait;
use tracing::{info, warn, error};
use chrono::{DateTime, Utc};
use serde::{Serialize, Deserialize};
use uuid::Uuid;

use bridge_core::{
    BridgeResult, 
    types::{ProcessedBatch, ProcessedRecord, TelemetryRecord, TelemetryType, TelemetryData, MetricData, MetricValue},
    traits::{LakehouseWriter, WriterStats},
    error::BridgeError,
};

use crate::config::DeltaLakeConfig;
use crate::error::{DeltaLakeError, DeltaLakeResult};

/// Delta Lake table metadata
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeltaTableMetadata {
    /// Table ID
    pub id: String,
    
    /// Table name
    pub name: String,
    
    /// Table description
    pub description: Option<String>,
    
    /// Table format version
    pub format_version: u32,
    
    /// Table schema
    pub schema: DeltaTableSchema,
    
    /// Table partition columns
    pub partition_columns: Vec<String>,
    
    /// Table configuration
    pub configuration: HashMap<String, String>,
    
    /// Created timestamp
    pub created_time: DateTime<Utc>,
    
    /// Last modified timestamp
    pub last_modified_time: DateTime<Utc>,
}

/// Delta Lake table schema
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeltaTableSchema {
    /// Schema fields
    pub fields: Vec<DeltaSchemaField>,
    
    /// Schema metadata
    pub metadata: HashMap<String, String>,
}

/// Delta Lake schema field
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeltaSchemaField {
    /// Field name
    pub name: String,
    
    /// Field type
    pub field_type: String,
    
    /// Field nullable
    pub nullable: bool,
    
    /// Field metadata
    pub metadata: HashMap<String, String>,
}

/// Delta Lake transaction log entry
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeltaTransactionLogEntry {
    /// Transaction ID
    pub txn_id: String,
    
    /// Transaction timestamp
    pub timestamp: DateTime<Utc>,
    
    /// Transaction type
    pub txn_type: String,
    
    /// Transaction data
    pub data: DeltaTransactionData,
}

/// Delta Lake transaction data
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeltaTransactionData {
    /// Added files
    pub add: Vec<DeltaFileAction>,
    
    /// Removed files
    pub remove: Vec<DeltaFileAction>,
    
    /// Metadata changes
    pub metaData: Option<DeltaTableMetadata>,
}

/// Delta Lake file action
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeltaFileAction {
    /// File path
    pub path: String,
    
    /// File size
    pub size: u64,
    
    /// File modification time
    pub modificationTime: DateTime<Utc>,
    
    /// File data change
    pub dataChange: bool,
    
    /// File statistics
    pub stats: Option<String>,
    
    /// File tags
    pub tags: HashMap<String, String>,
}

/// Real Delta Lake writer implementation
pub struct RealDeltaLakeWriter {
    /// Delta Lake configuration
    config: DeltaLakeConfig,
    
    /// Table metadata
    metadata: Arc<RwLock<DeltaTableMetadata>>,
    
    /// Transaction log
    transaction_log: Arc<RwLock<Vec<DeltaTransactionLogEntry>>>,
    
    /// Writer statistics
    stats: Arc<RwLock<WriterStats>>,
    
    /// Writer state
    is_initialized: Arc<RwLock<bool>>,
    
    /// Current transaction ID
    current_txn_id: Arc<RwLock<u64>>,
}

impl RealDeltaLakeWriter {
    /// Create a new Delta Lake writer
    pub fn new(config: DeltaLakeConfig) -> Self {
        let metadata = DeltaTableMetadata {
            id: Uuid::new_v4().to_string(),
            name: config.table_name.clone(),
            description: Some("OpenTelemetry Data Lake Bridge Delta Lake Table".to_string()),
            format_version: 2,
            schema: DeltaTableSchema {
                fields: vec![
                    DeltaSchemaField {
                        name: "id".to_string(),
                        field_type: "string".to_string(),
                        nullable: false,
                        metadata: HashMap::new(),
                    },
                    DeltaSchemaField {
                        name: "timestamp".to_string(),
                        field_type: "timestamp".to_string(),
                        nullable: false,
                        metadata: HashMap::new(),
                    },
                    DeltaSchemaField {
                        name: "record_type".to_string(),
                        field_type: "string".to_string(),
                        nullable: false,
                        metadata: HashMap::new(),
                    },
                    DeltaSchemaField {
                        name: "metric_name".to_string(),
                        field_type: "string".to_string(),
                        nullable: true,
                        metadata: HashMap::new(),
                    },
                    DeltaSchemaField {
                        name: "metric_value".to_string(),
                        field_type: "double".to_string(),
                        nullable: true,
                        metadata: HashMap::new(),
                    },
                    DeltaSchemaField {
                        name: "attributes".to_string(),
                        field_type: "string".to_string(),
                        nullable: true,
                        metadata: HashMap::new(),
                    },
                    DeltaSchemaField {
                        name: "tags".to_string(),
                        field_type: "string".to_string(),
                        nullable: true,
                        metadata: HashMap::new(),
                    },
                ],
                metadata: HashMap::new(),
            },
            partition_columns: vec!["record_type".to_string(), "date".to_string()],
            configuration: HashMap::new(),
            created_time: Utc::now(),
            last_modified_time: Utc::now(),
        };

        let stats = WriterStats {
            total_writes: 0,
            total_records: 0,
            total_bytes: 0,
            avg_write_time_ms: 0.0,
            error_count: 0,
            last_write_time: None,
        };

        Self {
            config,
            metadata: Arc::new(RwLock::new(metadata)),
            transaction_log: Arc::new(RwLock::new(Vec::new())),
            stats: Arc::new(RwLock::new(stats)),
            is_initialized: Arc::new(RwLock::new(false)),
            current_txn_id: Arc::new(RwLock::new(0)),
        }
    }

    /// Initialize the Delta Lake table
    pub async fn initialize(&mut self) -> DeltaLakeResult<()> {
        info!("Initializing Delta Lake table: {}", self.config.table_name);
        
        // Create table directory if it doesn't exist
        let table_path = Path::new(&self.config.table_path);
        if !table_path.exists() {
            tokio::fs::create_dir_all(table_path).await
                .map_err(|e| DeltaLakeError::InitializationError(format!("Failed to create table directory: {}", e)))?;
        }
        
        // Create _delta_log directory
        let delta_log_path = table_path.join("_delta_log");
        if !delta_log_path.exists() {
            tokio::fs::create_dir_all(&delta_log_path).await
                .map_err(|e| DeltaLakeError::InitializationError(format!("Failed to create _delta_log directory: {}", e)))?;
        }
        
        // Write initial transaction log
        self.write_transaction_log().await?;
        
        // Mark as initialized
        {
            let mut is_initialized = self.is_initialized.write().await;
            *is_initialized = true;
        }
        
        info!("Delta Lake table initialized successfully: {}", self.config.table_name);
        Ok(())
    }

    /// Write transaction log entry
    async fn write_transaction_log(&self) -> DeltaLakeResult<()> {
        let txn_id = {
            let mut current_txn_id = self.current_txn_id.write().await;
            *current_txn_id += 1;
            *current_txn_id
        };
        
        let entry = DeltaTransactionLogEntry {
            txn_id: format!("{:020}", txn_id),
            timestamp: Utc::now(),
            txn_type: "commit".to_string(),
            data: DeltaTransactionData {
                add: Vec::new(),
                remove: Vec::new(),
                metaData: None,
            },
        };
        
        // Add to transaction log
        {
            let mut transaction_log = self.transaction_log.write().await;
            transaction_log.push(entry.clone());
        }
        
        // Write transaction log file
        let table_path = Path::new(&self.config.table_path);
        let delta_log_path = table_path.join("_delta_log");
        let log_file_path = delta_log_path.join(format!("{:020}.json", txn_id));
        
        let log_content = serde_json::to_string_pretty(&entry)
            .map_err(|e| DeltaLakeError::WriteError(format!("Failed to serialize transaction log: {}", e)))?;
        
        tokio::fs::write(&log_file_path, log_content).await
            .map_err(|e| DeltaLakeError::WriteError(format!("Failed to write transaction log: {}", e)))?;
        
        info!("Wrote transaction log entry: {}", log_file_path.display());
        Ok(())
    }

    /// Convert telemetry record to Delta Lake format
    fn convert_record_to_delta_format(&self, record: &ProcessedRecord) -> HashMap<String, serde_json::Value> {
        let mut delta_record = HashMap::new();
        
        // Basic fields
        delta_record.insert("id".to_string(), serde_json::Value::String(record.original_id.to_string()));
        delta_record.insert("timestamp".to_string(), serde_json::Value::String(Utc::now().to_rfc3339()));
        
        // Extract data from transformed record
        if let Some(telemetry_data) = &record.transformed_data {
            match telemetry_data {
                TelemetryData::Metric(metric_data) => {
                    delta_record.insert("record_type".to_string(), serde_json::Value::String("metric".to_string()));
                    delta_record.insert("metric_name".to_string(), serde_json::Value::String(metric_data.name.clone()));
                    
                    let metric_value = match &metric_data.value {
                        MetricValue::Gauge(value) => *value,
                        MetricValue::Counter(value) => *value,
                        MetricValue::Histogram(value) => *value,
                    };
                    delta_record.insert("metric_value".to_string(), serde_json::Value::Number(serde_json::Number::from_f64(metric_value).unwrap_or(serde_json::Number::from(0))));
                }
                _ => {
                    delta_record.insert("record_type".to_string(), serde_json::Value::String("unknown".to_string()));
                }
            }
        }
        
        // Convert metadata
        if !record.metadata.is_empty() {
            let metadata_json = serde_json::to_value(&record.metadata)
                .unwrap_or(serde_json::Value::Object(serde_json::Map::new()));
            delta_record.insert("attributes".to_string(), metadata_json);
        }
        
        delta_record
    }

    /// Write batch to Delta Lake table
    async fn write_batch_to_delta(&self, batch: &ProcessedBatch) -> DeltaLakeResult<()> {
        let start_time = std::time::Instant::now();
        
        // Convert records to Delta Lake format
        let mut delta_records = Vec::new();
        for record in &batch.records {
            let delta_record = self.convert_record_to_delta_format(record);
            delta_records.push(delta_record);
        }
        
        // Create data file
        let table_path = Path::new(&self.config.table_path);
        let file_name = format!("part-{:020}-{}.json", 
            self.current_txn_id.read().await, 
            Uuid::new_v4().to_string());
        let file_path = table_path.join(&file_name);
        
        // Write data file
        let file_content = serde_json::to_string_pretty(&delta_records)
            .map_err(|e| DeltaLakeError::WriteError(format!("Failed to serialize data file: {}", e)))?;
        
        tokio::fs::write(&file_path, file_content).await
            .map_err(|e| DeltaLakeError::WriteError(format!("Failed to write data file: {}", e)))?;
        
        // Update transaction log with file addition
        let file_size = tokio::fs::metadata(&file_path).await
            .map(|m| m.len())
            .unwrap_or(0);
        
        let file_action = DeltaFileAction {
            path: file_name,
            size: file_size,
            modificationTime: Utc::now(),
            dataChange: true,
            stats: None,
            tags: HashMap::new(),
        };
        
        // Update transaction log
        {
            let mut transaction_log = self.transaction_log.write().await;
            if let Some(last_entry) = transaction_log.last_mut() {
                last_entry.data.add.push(file_action);
            }
        }
        
        // Write updated transaction log
        self.write_transaction_log().await?;
        
        // Update statistics
        {
            let mut stats = self.stats.write().await;
            stats.total_writes += 1;
            stats.total_records += batch.records.len() as u64;
            stats.total_bytes += file_size;
            stats.last_write_time = Some(Utc::now());
            
            let duration_ms = start_time.elapsed().as_millis() as f64;
            if stats.total_writes > 1 {
                stats.avg_write_time_ms = 
                    (stats.avg_write_time_ms * (stats.total_writes - 1) as f64 + duration_ms) 
                    / stats.total_writes as f64;
            } else {
                stats.avg_write_time_ms = duration_ms;
            }
        }
        
        info!("Wrote batch to Delta Lake table: {} records, {} bytes", 
              batch.records.len(), file_size);
        
        Ok(())
    }
}

#[async_trait]
impl LakehouseWriter for RealDeltaLakeWriter {
    async fn write(&self, batch: ProcessedBatch) -> BridgeResult<()> {
        // Check if writer is initialized
        if !*self.is_initialized.read().await {
            return Err(BridgeError::internal("Delta Lake writer is not initialized"));
        }
        
        // Write batch to Delta Lake
        self.write_batch_to_delta(&batch).await
            .map_err(|e| BridgeError::write(e.to_string()))?;
        
        Ok(())
    }
    
    async fn health_check(&self) -> BridgeResult<bool> {
        Ok(*self.is_initialized.read().await)
    }
    
    async fn get_stats(&self) -> BridgeResult<WriterStats> {
        let stats = self.stats.read().await;
        Ok(stats.clone())
    }
    
    async fn shutdown(&self) -> BridgeResult<()> {
        info!("Shutting down Delta Lake writer");
        
        // Write final transaction log entry
        self.write_transaction_log().await
            .map_err(|e| BridgeError::internal(format!("Failed to write final transaction log: {}", e)))?;
        
        info!("Delta Lake writer shutdown completed");
        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::tempdir;

    #[tokio::test]
    async fn test_delta_lake_writer_creation() {
        let config = DeltaLakeConfig {
            table_name: "test_table".to_string(),
            table_path: "/tmp/test_delta_table".to_string(),
            ..Default::default()
        };
        
        let writer = RealDeltaLakeWriter::new(config);
        assert_eq!(writer.config.table_name, "test_table");
    }

    #[tokio::test]
    async fn test_delta_lake_writer_initialization() {
        let temp_dir = tempdir().unwrap();
        let config = DeltaLakeConfig {
            table_name: "test_table".to_string(),
            table_path: temp_dir.path().join("test_delta_table").to_string_lossy().to_string(),
            ..Default::default()
        };
        
        let mut writer = RealDeltaLakeWriter::new(config);
        let result = writer.initialize().await;
        assert!(result.is_ok());
        
        // Check that table directory was created
        let table_path = Path::new(&writer.config.table_path);
        assert!(table_path.exists());
        assert!(table_path.join("_delta_log").exists());
    }

    #[tokio::test]
    async fn test_delta_lake_writer_write() {
        let temp_dir = tempdir().unwrap();
        let config = DeltaLakeConfig {
            table_name: "test_table".to_string(),
            table_path: temp_dir.path().join("test_delta_table").to_string_lossy().to_string(),
            ..Default::default()
        };
        
        let mut writer = RealDeltaLakeWriter::new(config);
        writer.initialize().await.unwrap();
        
        // Create a test batch
        let record = TelemetryRecord {
            id: Uuid::new_v4(),
            timestamp: Utc::now(),
            record_type: TelemetryType::Metric,
            data: TelemetryData::Metric(MetricData {
                name: "test_metric".to_string(),
                description: Some("Test metric".to_string()),
                unit: Some("count".to_string()),
                metric_type: bridge_core::types::MetricType::Gauge,
                value: MetricValue::Gauge(42.0),
                labels: HashMap::new(),
                timestamp: Utc::now(),
            }),
            attributes: HashMap::new(),
            tags: HashMap::new(),
            resource: None,
            service: None,
        };
        
        let processed_record = ProcessedRecord {
            original_id: Uuid::new_v4(),
            status: bridge_core::types::ProcessingStatus::Success,
            transformed_data: Some(record.data.clone()),
            metadata: HashMap::new(),
            errors: Vec::new(),
        };
        
        let batch = ProcessedBatch {
            original_batch_id: Uuid::new_v4(),
            timestamp: Utc::now(),
            records: vec![processed_record],
            metadata: HashMap::new(),
            status: bridge_core::types::ProcessingStatus::Success,
            errors: Vec::new(),
        };
        
        // Write the batch
        let result = writer.write(batch).await;
        assert!(result.is_ok());
        
        // Check that data file was created
        let table_path = Path::new(&writer.config.table_path);
        let files: Vec<_> = tokio::fs::read_dir(table_path).await.unwrap()
            .filter_map(|entry| entry.ok())
            .filter(|entry| entry.file_name().to_string_lossy().starts_with("part-"))
            .collect();
        
        assert!(!files.is_empty());
    }
}
